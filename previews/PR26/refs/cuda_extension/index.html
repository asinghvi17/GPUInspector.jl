<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>CUDA Extension · GPUInspector.jl</title><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.svg" alt="GPUInspector.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">GPUInspector.jl</a></span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">GPUInspector</a></li><li><input class="collapse-toggle" id="menuitem-2" type="checkbox"/><label class="tocitem" for="menuitem-2"><span class="docs-label">Examples</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../examples/gpuinfo/">GPU Information</a></li><li><a class="tocitem" href="../../examples/data_bandwidth/">Data Bandwidth</a></li><li><a class="tocitem" href="../../examples/peakflops_gpu/">Peakflops</a></li><li><a class="tocitem" href="../../examples/gpustresstest/">GPU Stress Test</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-3" type="checkbox"/><label class="tocitem" for="menuitem-3"><span class="docs-label">Explanations</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../explanations/dgx/">DGX Details</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-4" type="checkbox" checked/><label class="tocitem" for="menuitem-4"><span class="docs-label">References</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../gpuinfo/">GPU Information</a></li><li><a class="tocitem" href="../data_bandwidth/">Data Bandwidth</a></li><li><a class="tocitem" href="../peakflops_gpu/">Peakflops</a></li><li><a class="tocitem" href="../gpustresstest/">GPU Stress Test</a></li><li><a class="tocitem" href="../stresstest_cpu/">CPU Stress Test</a></li><li><a class="tocitem" href="../monitoring/">GPU Monitoring</a></li><li><a class="tocitem" href="../backends/">Backends</a></li><li class="is-active"><a class="tocitem" href>CUDA Extension</a><ul class="internal"><li><a class="tocitem" href="#Index"><span>Index</span></a></li></ul></li><li><a class="tocitem" href="../utility/">Utility</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-5" type="checkbox"/><label class="tocitem" for="menuitem-5"><span class="docs-label">Tested Devices</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../devices/a100_sxm2/">A100 SXM2</a></li><li><a class="tocitem" href="../../devices/v100_sxm2/">V100 SXM2</a></li><li><a class="tocitem" href="../../devices/geforce_gtx_1650/">GeForce GTX 1650</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">References</a></li><li class="is-active"><a href>CUDA Extension</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>CUDA Extension</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com//blob/main/docs/src/refs/cuda_extension.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="CUDA-Extension"><a class="docs-heading-anchor" href="#CUDA-Extension">CUDA Extension</a><a id="CUDA-Extension-1"></a><a class="docs-heading-anchor-permalink" href="#CUDA-Extension" title="Permalink"></a></h1><h2 id="Index"><a class="docs-heading-anchor" href="#Index">Index</a><a id="Index-1"></a><a class="docs-heading-anchor-permalink" href="#Index" title="Permalink"></a></h2><ul><li><a href="#CUDAExt._kernel_fma-NTuple{4, Any}"><code>CUDAExt._kernel_fma</code></a></li><li><a href="#CUDAExt._peakflops_gpu_fmas-Tuple{}"><code>CUDAExt._peakflops_gpu_fmas</code></a></li><li><a href="#CUDAExt._peakflops_gpu_wmmas-Tuple{}"><code>CUDAExt._peakflops_gpu_wmmas</code></a></li><li><a href="#CUDAExt.alloc_mem-Tuple{UnitPrefixedBytes}"><code>CUDAExt.alloc_mem</code></a></li><li><a href="#CUDAExt.get_gpu_utilization"><code>CUDAExt.get_gpu_utilization</code></a></li><li><a href="#CUDAExt.get_gpu_utilizations"><code>CUDAExt.get_gpu_utilizations</code></a></li><li><a href="#CUDAExt.get_power_usage-Tuple{CUDA.NVML.Device}"><code>CUDAExt.get_power_usage</code></a></li><li><a href="#CUDAExt.get_power_usages"><code>CUDAExt.get_power_usages</code></a></li><li><a href="#CUDAExt.get_temperature"><code>CUDAExt.get_temperature</code></a></li><li><a href="#CUDAExt.get_temperatures"><code>CUDAExt.get_temperatures</code></a></li><li><a href="#CUDAExt.gpuid"><code>CUDAExt.gpuid</code></a></li><li><a href="#CUDAExt.hastensorcores"><code>CUDAExt.hastensorcores</code></a></li><li><a href="#CUDAExt.peakflops_gpu_matmul-Tuple{}"><code>CUDAExt.peakflops_gpu_matmul</code></a></li><li><a href="#CUDAExt.peakflops_gpu_matmul_graphs-Tuple{}"><code>CUDAExt.peakflops_gpu_matmul_graphs</code></a></li><li><a href="#CUDAExt.peakflops_gpu_matmul_scaling-Union{Tuple{}, Tuple{F}} where F"><code>CUDAExt.peakflops_gpu_matmul_scaling</code></a></li><li><a href="#CUDAExt.toggle_tensorcoremath"><code>CUDAExt.toggle_tensorcoremath</code></a></li><li><a href="#GPUInspector.gpuinfo-Tuple{NVIDIABackend, Integer}"><code>GPUInspector.gpuinfo</code></a></li><li><a href="#GPUInspector.memory_bandwidth_saxpy-Tuple{NVIDIABackend}"><code>GPUInspector.memory_bandwidth_saxpy</code></a></li><li><a href="#GPUInspector.monitoring_stop-Tuple{NVIDIABackend}"><code>GPUInspector.monitoring_stop</code></a></li><li><a href="#CUDAExt.StressTestBatched"><code>CUDAExt.StressTestBatched</code></a></li><li><a href="#CUDAExt.StressTestEnforced"><code>CUDAExt.StressTestEnforced</code></a></li><li><a href="#CUDAExt.StressTestFixedIter"><code>CUDAExt.StressTestFixedIter</code></a></li><li><a href="#CUDAExt.StressTestStoreResults"><code>CUDAExt.StressTestStoreResults</code></a></li></ul><h3 id="References"><a class="docs-heading-anchor" href="#References">References</a><a id="References-1"></a><a class="docs-heading-anchor-permalink" href="#References" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="CUDAExt.get_gpu_utilization" href="#CUDAExt.get_gpu_utilization"><code>CUDAExt.get_gpu_utilization</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">get_gpu_utilization(device=CUDA.device())</code></pre><p>Get the current utilization of the given CUDA device in percent.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="CUDAExt.get_gpu_utilizations" href="#CUDAExt.get_gpu_utilizations"><code>CUDAExt.get_gpu_utilizations</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">get_gpu_utilizations(devices=CUDA.devices())</code></pre><p>Get the current utilization of the given CUDA devices in percent.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="CUDAExt.get_power_usage-Tuple{CUDA.NVML.Device}" href="#CUDAExt.get_power_usage-Tuple{CUDA.NVML.Device}"><code>CUDAExt.get_power_usage</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">get_power_usage(device=CUDA.device())</code></pre><p>Get current power usage of the given CUDA device in Watts.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="CUDAExt.get_power_usages" href="#CUDAExt.get_power_usages"><code>CUDAExt.get_power_usages</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">get_power_usages(devices=CUDA.devices())</code></pre><p>Get current power usage of the given CUDA devices in Watts.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="CUDAExt.get_temperature" href="#CUDAExt.get_temperature"><code>CUDAExt.get_temperature</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">get_temperature(device=CUDA.device())</code></pre><p>Get current temperature of the given CUDA device in degrees Celsius.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="CUDAExt.get_temperatures" href="#CUDAExt.get_temperatures"><code>CUDAExt.get_temperatures</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">get_temperatures(devices=CUDA.devices())</code></pre><p>Get current temperature of the given CUDA devices in degrees Celsius.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="CUDAExt.gpuid" href="#CUDAExt.gpuid"><code>CUDAExt.gpuid</code></a> — <span class="docstring-category">Function</span></header><section><div><p>Get GPU index of the given device.</p><p><strong>Note:</strong> GPU indices start with zero.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="CUDAExt._kernel_fma-NTuple{4, Any}" href="#CUDAExt._kernel_fma-NTuple{4, Any}"><code>CUDAExt._kernel_fma</code></a> — <span class="docstring-category">Method</span></header><section><div><p>Dummy kernel doing <code>_kernel_fma_nfmas()</code> many FMAs (default: <code>100_000</code>).</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="CUDAExt._peakflops_gpu_fmas-Tuple{}" href="#CUDAExt._peakflops_gpu_fmas-Tuple{}"><code>CUDAExt._peakflops_gpu_fmas</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">_peakflops_gpu_fmas(; size::Integer=5_000_000, dtype=Float32, nbench=5, nkernel=5, device=CUDA.device(), verbose=true)</code></pre><p>Tries to estimate the peak performance of a GPU in TFLOP/s by measuring the time it takes to perform <code>_kernel_fma_nfmas() * size</code> many FMAs on CUDA cores.</p><p><strong>Keyword arguments:</strong></p><ul><li><code>device</code> (default: <code>CUDA.device()</code>): CUDA device to be used.</li><li><code>dtype</code> (default: <code>Float32</code>): element type of the matrices.</li><li><code>size</code> (default: <code>5_000_000</code>): length of vectors.</li><li><code>nkernel</code> (default: <code>5</code>): number of kernel calls that make up one benchmarking sample.</li><li><code>nbench</code> (default: <code>5</code>): number of measurements to be performed the best of which is used for the TFLOP/s computation.</li><li><code>verbose</code> (default: <code>true</code>): toggle printing.</li><li><code>io</code> (default: <code>stdout</code>): set the stream where the results should be printed.</li></ul></div></section></article><article class="docstring"><header><a class="docstring-binding" id="CUDAExt._peakflops_gpu_wmmas-Tuple{}" href="#CUDAExt._peakflops_gpu_wmmas-Tuple{}"><code>CUDAExt._peakflops_gpu_wmmas</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">_peakflops_gpu_wmmas()</code></pre><p>Tries to estimate the peak performance of a GPU in TFLOP/s by measuring the time it takes to perform <code>_kernel_wmma_nwmmas()</code> many WMMAs on Tensor Cores.</p><p><strong>Keyword arguments:</strong></p><ul><li><code>device</code> (default: <code>CUDA.device()</code>): CUDA device to be used.</li><li><code>dtype</code> (default: <code>Float16</code>): element type of the matrices. We currently only support <code>Float16</code> (<code>Int8</code>, <code>:TensorFloat32</code>, <code>:BFloat16</code>, and <code>Float64</code> might or might not work).</li><li><code>nkernel</code> (default: <code>10</code>): number of kernel calls that make up one benchmarking sample.</li><li><code>nbench</code> (default: <code>5</code>): number of measurements to be performed the best of which is used for the TFLOP/s computation.</li><li><code>threads</code> (default: max. threads per block): how many threads to use per block (part of the kernel launch configuration).</li><li><code>blocks</code> (default: <code>2048</code>): how many blocks to use (part of the kernel launch configuration).</li><li><code>verbose</code> (default: <code>true</code>): toggle printing.</li><li><code>io</code> (default: <code>stdout</code>): set the stream where the results should be printed.</li></ul></div></section></article><article class="docstring"><header><a class="docstring-binding" id="CUDAExt.peakflops_gpu_matmul-Tuple{}" href="#CUDAExt.peakflops_gpu_matmul-Tuple{}"><code>CUDAExt.peakflops_gpu_matmul</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">peakflops_gpu_matmul(; device, dtype=Float32, size=2^14, nmatmuls=5, nbench=5, verbose=true)</code></pre><p>Tries to estimate the peak performance of a GPU in TFLOP/s by measuring the time it takes to perform <code>nmatmuls</code> many (in-place) matrix-matrix multiplications.</p><p><strong>Keyword arguments:</strong></p><ul><li><code>device</code> (default: <code>CUDA.device()</code>): CUDA device to be used.</li><li><code>dtype</code> (default: <code>Float32</code>): element type of the matrices.</li><li><code>size</code> (default: <code>2^14</code>): matrices will have dimensions <code>(size, size)</code>.</li><li><code>nmatmuls</code> (default: <code>5</code>): number of matmuls that will make up the kernel to be timed.</li><li><code>nbench</code> (default: <code>5</code>): number of measurements to be performed the best of which is used for the TFLOP/s computation.</li><li><code>verbose</code> (default: <code>true</code>): toggle printing.</li><li><code>io</code> (default: <code>stdout</code>): set the stream where the results should be printed.</li></ul><p>See also: <a href="#CUDAExt.peakflops_gpu_matmul_scaling-Union{Tuple{}, Tuple{F}} where F"><code>peakflops_gpu_matmul_scaling</code></a>, <a href="#CUDAExt.peakflops_gpu_matmul_graphs-Tuple{}"><code>peakflops_gpu_matmul_graphs</code></a>.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="CUDAExt.peakflops_gpu_matmul_graphs-Tuple{}" href="#CUDAExt.peakflops_gpu_matmul_graphs-Tuple{}"><code>CUDAExt.peakflops_gpu_matmul_graphs</code></a> — <span class="docstring-category">Method</span></header><section><div><p>Same as <a href="#CUDAExt.peakflops_gpu_matmul-Tuple{}"><code>peakflops_gpu_matmul</code></a> but uses CUDA&#39;s graph API to define and launch the kernel.</p><p>See also: <a href="#CUDAExt.peakflops_gpu_matmul_scaling-Union{Tuple{}, Tuple{F}} where F"><code>peakflops_gpu_matmul_scaling</code></a>.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="CUDAExt.peakflops_gpu_matmul_scaling-Union{Tuple{}, Tuple{F}} where F" href="#CUDAExt.peakflops_gpu_matmul_scaling-Union{Tuple{}, Tuple{F}} where F"><code>CUDAExt.peakflops_gpu_matmul_scaling</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">peakflops_gpu_matmul_scaling(peakflops_func = peakflops_gpu_matmul; verbose=true) -&gt; sizes, flops</code></pre><p>Asserts the scaling of the given <code>peakflops_func</code>tion (defaults to <a href="#CUDAExt.peakflops_gpu_matmul-Tuple{}"><code>peakflops_gpu_matmul</code></a>) with increasing matrix size. If <code>verbose=true</code> (default), displays a unicode plot. Returns the considered sizes and TFLOP/s. For further options, see <a href="#CUDAExt.peakflops_gpu_matmul-Tuple{}"><code>peakflops_gpu_matmul</code></a>.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="CUDAExt.StressTestBatched" href="#CUDAExt.StressTestBatched"><code>CUDAExt.StressTestBatched</code></a> — <span class="docstring-category">Type</span></header><section><div><p>GPU stress test (matrix multiplications) in which we try to run for a given time period. We try to keep the CUDA stream continously busy with matmuls at any point in time. Concretely, we submit batches of matmuls and, after half of them, we record a CUDA event. On the host, after submitting a batch, we (non-blockingly) synchronize on, i.e. wait for, the CUDA event and, if we haven&#39;t exceeded the desired duration already, submit another batch.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="CUDAExt.StressTestEnforced" href="#CUDAExt.StressTestEnforced"><code>CUDAExt.StressTestEnforced</code></a> — <span class="docstring-category">Type</span></header><section><div><p>GPU stress test (matrix multiplications) in which we run almost precisely for a given time period (duration is enforced).</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="CUDAExt.StressTestFixedIter" href="#CUDAExt.StressTestFixedIter"><code>CUDAExt.StressTestFixedIter</code></a> — <span class="docstring-category">Type</span></header><section><div><p>GPU stress test (matrix multiplications) in which we run for a given number of iteration, or try to run for a given time period (with potentially high uncertainty!). In the latter case, we estimate how long a synced matmul takes and set <code>niter</code> accordingly.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="CUDAExt.StressTestStoreResults" href="#CUDAExt.StressTestStoreResults"><code>CUDAExt.StressTestStoreResults</code></a> — <span class="docstring-category">Type</span></header><section><div><p>GPU stress test (matrix multiplications) in which we store all matmul results and try to run as many iterations as possible for a certain memory limit (default: 90% of free memory).</p><p>This stress test is somewhat inspired by <a href="https://github.com/wilicc/gpu-burn">gpu-burn</a> by Ville Timonen.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="CUDAExt.alloc_mem-Tuple{UnitPrefixedBytes}" href="#CUDAExt.alloc_mem-Tuple{UnitPrefixedBytes}"><code>CUDAExt.alloc_mem</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">alloc_mem(memsize::UnitPrefixedBytes; devs=(CUDA.device(),), dtype=Float32)</code></pre><p>Allocates memory on the devices whose IDs are provided via <code>devs</code>. Returns a vector of memory handles (i.e. <code>CuArray</code>s).</p><p><strong>Examples:</strong></p><pre><code class="language-julia hljs">alloc_mem(MiB(1024)) # allocate on the currently active device
alloc_mem(B(40_000_000); devs=(0,1)) # allocate on GPU0 and GPU1</code></pre></div></section></article><article class="docstring"><header><a class="docstring-binding" id="CUDAExt.hastensorcores" href="#CUDAExt.hastensorcores"><code>CUDAExt.hastensorcores</code></a> — <span class="docstring-category">Function</span></header><section><div><p>Checks whether the given <code>CuDevice</code> has Tensor Cores.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="CUDAExt.toggle_tensorcoremath" href="#CUDAExt.toggle_tensorcoremath"><code>CUDAExt.toggle_tensorcoremath</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">toggle_tensorcoremath([enable::Bool; verbose=true])</code></pre><p>Switches the <code>CUDA.math_mode</code> between <code>CUDA.FAST_MATH</code> (<code>enable=true</code>) and <code>CUDA.DEFAULT_MATH</code> (<code>enable=false</code>). For matmuls of <code>CuArray{Float32}</code>s, this should have the effect of using/enabling and not using/disabling tensor cores. Of course, this only works on supported devices and CUDA versions.</p><p>If no arguments are provided, this functions toggles between the two math modes.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="GPUInspector.memory_bandwidth_saxpy-Tuple{NVIDIABackend}" href="#GPUInspector.memory_bandwidth_saxpy-Tuple{NVIDIABackend}"><code>GPUInspector.memory_bandwidth_saxpy</code></a> — <span class="docstring-category">Method</span></header><section><div><p>Extra keyword arguments:</p><ul><li><code>cublas</code> (default: <code>true</code>): toggle between <code>CUDA.axpy!</code> and a custom <code>_saxpy_gpu_kernel!</code>.</li></ul><p>(This method is from the NVIDIA Backend.)</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="GPUInspector.gpuinfo-Tuple{NVIDIABackend, Integer}" href="#GPUInspector.gpuinfo-Tuple{NVIDIABackend, Integer}"><code>GPUInspector.gpuinfo</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">gpuinfo(deviceid::Integer)</code></pre><p>Print out detailed information about the NVIDIA GPU with the given <code>deviceid</code>.</p><p>Heavily inspired by the CUDA sample &quot;deviceQueryDrv.cpp&quot;.</p><p>(This method is from the NVIDIA Backend.)</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="GPUInspector.monitoring_stop-Tuple{NVIDIABackend}" href="#GPUInspector.monitoring_stop-Tuple{NVIDIABackend}"><code>GPUInspector.monitoring_stop</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">monitoring_stop(; verbose=true) -&gt; results</code></pre><p>Specifically, <code>results</code> is a named tuple with the following keys:</p><ul><li><code>time</code>: the (relative) times at which we measured</li><li><code>temperature</code>, <code>power</code>, <code>compute</code>, <code>mem</code></li></ul><p>(This method is from the NVIDIA Backend.)</p></div></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../backends/">« Backends</a><a class="docs-footer-nextpage" href="../utility/">Utility »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.25 on <span class="colophon-date" title="Thursday 17 August 2023 15:58">Thursday 17 August 2023</span>. Using Julia version 1.9.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
