# NVIDIA A40 (sm_86, 44.988 GiB)

Benchmarks were run on a [noctua1](@ref) at [PC2](https://pc2.uni-paderborn.de). 

For comparison: [Datasheet](https://images.nvidia.com/content/Solutions/data-center/a40/nvidia-a40-datasheet.pdf) by NVIDIA.

## Peakflops

### CUDA cores

```julia
julia> theoretical_peakflops_gpu(; dtype=Float32, tensorcores=false);
Theoretical Peakflops (TFLOP/s):
 ├ tensorcores: false
 ├ dtype: Float32
 └ max: 37.4

julia> theoretical_peakflops_gpu(; dtype=Float64, tensorcores=false);
Theoretical Peakflops (TFLOP/s):
 ├ tensorcores: false
 ├ dtype: Float64
 └ max: 18.7
```

```julia
julia> peakflops_gpu(; dtype=Float32, tensorcores=false);
Peakflops (TFLOP/s):
 ├ tensorcores: false
 ├ dtype: Float32
 └ max: 23.3

julia> peakflops_gpu(; dtype=Float64, tensorcores=false);
Peakflops (TFLOP/s):
 ├ tensorcores: false
 ├ dtype: Float64
 └ max: 0.6

julia> peakflops_gpu(; dtype=Float16, tensorcores=false);
Peakflops (TFLOP/s):
 ├ tensorcores: false
 ├ dtype: Float16
 └ max: 18.6
```

### Tensor cores

```julia
julia> theoretical_peakflops_gpu(; dtype=Int8, tensorcores=true);
...

julia> theoretical_peakflops_gpu(; dtype=Float16, tensorcores=true);
...

julia> theoretical_peakflops_gpu(; dtype=Float32, tensorcores=true);
...
```

```julia

julia> peakflops_gpu(; dtype=Int8, tensorcores=true);
Peakflops (TOP/s):
 ├ tensorcores: true
 ├ dtype: Int8
 └ max: 284.4

julia> peakflops_gpu(; dtype=Float16, tensorcores=true);
Peakflops (TFLOP/s):
 ├ tensorcores: true
 ├ dtype: Float16
 └ max: 145.9

julia> peakflops_gpu(; dtype=:TensorFloat32, tensorcores=true); # as of writing, only works with Julia >= 1.8.0 and CUDA.jl PR 1419
...

```

## Memory bandwidth

```julia
julia> theoretical_memory_bandwidth();
Theoretical Maximal Memory Bandwidth (GiB/s):
 └ max: 648.3

julia> memory_bandwidth();
Memory Bandwidth (GiB/s):
 └ max: 532.77

julia> GiB(1220.7) |> change_base
~1310.72 GB

julia> memory_bandwidth_saxpy();
Memory Bandwidth (GiB/s):
 └ max: 532.28
```

## Host-to-device bandwidth
```julia
julia> host2device_bandwidth()
Host <-> Device Bandwidth (GiB/s):
 └ max: 4.98

Host (pinned) <-> Device Bandwidth (GiB/s):
 └ max: 12.11
```

## Peer-to-peer bandwidth

```julia
julia> p2p_bandwidth();
Bandwidth (GiB/s):
 ├ max: 9.2
 ├ min: 0.63
 ├ avg: 7.48
 └ std_dev: 3.83

julia> p2p_bandwidth_all()
2×2 Matrix{Union{Nothing, Float64}}:
  nothing  9.20374
 8.51313    nothing
```

## GPU information

```julia
julia> CUDA.versioninfo()
CUDA toolkit 11.7, artifact installation
NVIDIA driver 535.54.3, for CUDA 12.2
CUDA driver 12.2

Libraries:
- CUBLAS: 11.10.1
- CURAND: 10.2.10
- CUFFT: 10.7.2
- CUSOLVER: 11.3.5
- CUSPARSE: 11.7.3
- CUPTI: 17.0.0
- NVML: 12.0.0+535.54.3
- CUDNN: 8.30.2 (for CUDA 11.5.0)
- CUTENSOR: 1.4.0 (for CUDA 11.5.0)

Toolchain:
- Julia: 1.9.2
- LLVM: 14.0.6
- PTX ISA support: 3.2, 4.0, 4.1, 4.2, 4.3, 5.0, 6.0, 6.1, 6.3, 6.4, 6.5, 7.0, 7.1, 7.2, 7.3, 7.4, 7.5
- Device capability support: sm_35, sm_37, sm_50, sm_52, sm_53, sm_60, sm_61, sm_62, sm_70, sm_72, sm_75, sm_80, sm_86

Environment:
- JULIA_CUDA_MEMORY_POOL: none

2 devices:
  0: NVIDIA A40 (sm_86, 43.509 GiB / 44.988 GiB available)
  1: NVIDIA A40 (sm_86, 44.081 GiB / 44.988 GiB available)

julia> gpuinfo()
Device: NVIDIA A40 (CuDevice(1))
Total amount of global memory: 44.352 GiB
Number of CUDA cores: 10752
Number of multiprocessors: 84 (128 CUDA cores each)
GPU max. clock rate: 1740 MHz
Memory clock rate: 7251 MHz
Memory bus width: 384-bit
L2 cache size: 6.000 MiB
Max. texture dimension sizes (1D): 131072
Max. texture dimension sizes (2D): 131072, 65536
Max. texture dimension sizes (3D): 16384, 16384, 16384
Max. layered 1D texture size: 32768 (2048 layers)
Max. layered 2D texture size: 32768, 32768 (2048 layers)
Total amount of constant memory: 64.000 KiB
Total amount of shared memory per block: 48.000 KiB
Total number of registers available per block: 65536
Warp size: 32
Max. number of threads per multiprocessor: 1536
Max. number of threads per block: 1024
Max. dimension size of a thread block (x,y,z): 1024, 1024, 64
Max. dimension size of a grid size (x,y,z): 2147483647, 65535, 65535
Texture alignment: 512 bytes
Maximum memory pitch: 2.000 GiB
Concurrent copy and kernel execution: Yes with 2 copy engine(s)
Run time limit on kernels: No
Integrated GPU sharing host memory: No
Support host page-locked memory mapping: Yes
Concurrent kernel execution: Yes
Alignment requirement for surfaces: Yes
Device has ECC support: Yes
Device supports Unified Addressing (UVA): Yes
Device supports managed memory: Yes
Device supports compute preemption: Yes
Supports cooperative kernel launch: Yes
Supports multi-device co-op kernel launch: Yes
Device PCI domain ID / bus ID / device ID: 0 / 175 / 0
Compute mode: Default (multiple host threads can use ::cudaSetDevice() with device simultaneously)

julia> gpuinfo_p2p_access()
P2P Access Supported:
2×2 Matrix{Bool}:
 0  1
 1  0

P2P Atomic Supported:
2×2 Matrix{Bool}:
 0  0
 0  0

```
